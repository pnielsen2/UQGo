{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pettingzoo.classic import go_v5\n",
    "from SMPyBandits import Policies\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixedMOSSEcomputeAllIndex(self):\n",
    "    \"\"\" Compute the current indexes for all arms, in a vectorized manner.\"\"\"\n",
    "    pulls_of_suboptimal_arms = np.sum(self.pulls[self.pulls < np.sqrt(self.t)])\n",
    "    if pulls_of_suboptimal_arms > 0:\n",
    "        indexes = (self.rewards / self.pulls) + np.sqrt(0.5 * np.maximum(0, np.log(self.t / pulls_of_suboptimal_arms)) / self.pulls)\n",
    "    else:\n",
    "        indexes = (self.rewards / self.pulls) + np.sqrt(0.5 * np.maximum(0, np.log(self.t / (self.nbArms * self.pulls))) / self.pulls)\n",
    "    # indexes[self.pulls < 1] = float('+inf')\n",
    "    self.index[:] = indexes\n",
    "Policies.MOSSExperimental.computeAllIndex = fixedMOSSEcomputeAllIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_policy = Policies.UCB(len(policy_algorithm_list))\n",
    "# squares = np.zeros(len(policy_algorithm_list))\n",
    "regrets = []\n",
    "while True:\n",
    "    # meta_choice = meta_policy.choice()\n",
    "    # meta_choice = np.argmin(meta_policy.pulls)\n",
    "    # print(f'choice:{meta_choice}')\n",
    "    # policy_algorithm = Policies.Thompson()\n",
    "    num_arms = 10\n",
    "    num_fictitious_pulls = 1000\n",
    "    random_fictitious = False\n",
    "    if random_fictitious:\n",
    "        fictitious_pulls = np.random.dirichlet(num_arms*[1])*num_fictitious_pulls\n",
    "        fictitious_empirical_means = np.random.random(num_arms)\n",
    "    else:\n",
    "        fictitious_pulls = np.array(num_arms*[0])\n",
    "        fictitious_empirical_means = np.array(num_arms*[0])\n",
    "\n",
    "    # print(fictitious_pulls)\n",
    "\n",
    "    fictitious_rewards = fictitious_pulls * fictitious_empirical_means\n",
    "    # print(fictitious_rewards)\n",
    "    # policy = Policies.UCBoost_bq_h_lb(num_arms)\n",
    "    policy = Policies.Thompson(num_arms)\n",
    "    # policy = CPUCB(num_arms)\n",
    "    # policy = klUCB(num_arms)\n",
    "    policy.pulls = fictitious_pulls\n",
    "    # print(policy.pulls)\n",
    "    policy.rewards = fictitious_rewards\n",
    "    # true_probs = np.random.random(num_arms)\n",
    "    # true_probs = np.array([.5]* num_arms)\n",
    "    true_probs = scipy.stats.beta.rvs(50,50,size=num_arms)\n",
    "    # print(true_probs)\n",
    "\n",
    "    for i in range(10000):\n",
    "        # print(policy.index)\n",
    "        action = policy.choice()\n",
    "        reward = np.random.binomial(1,true_probs[action])\n",
    "        policy.getReward(action, reward)\n",
    "        # policy.pulls[action]+=1\n",
    "        # policy.rewards[action]+=reward\n",
    "        # print(action,reward)\n",
    "        # print(policy.pulls)\n",
    "        # policy.computeAllIndex()\n",
    "    # print(true_probs)\n",
    "    # print(true_probs.round(4))\n",
    "    print('pulls:')\n",
    "    print(policy.pulls)\n",
    "    print('rewards:')\n",
    "    print(policy.rewards)\n",
    "    # print(policy.rewards/policy.pulls)\n",
    "    regret = policy.pulls.sum()*true_probs.max()-(policy.pulls*true_probs).sum()\n",
    "    print(f'regret:{regret}')\n",
    "    regrets.append(regret)\n",
    "    # meta_policy.getReward(meta_choice, -regret)\n",
    "    # squares[meta_choice]+=regret**2\n",
    "    # print('meta mean rewards')\n",
    "    # print(meta_policy.rewards/(meta_policy.pulls+.0001))\n",
    "    # print('meta pulls:')\n",
    "    # score = -meta_policy.rewards/(meta_policy.pulls+.0001)\n",
    "    # stdev = np.sqrt(squares/(meta_policy.pulls+.0001)-score**2)\n",
    "    # mean_stdev = stdev/np.sqrt((meta_policy.pulls+.0001))\n",
    "    # print((pd.DataFrame({'Name':policy_algorithm_names, 'score': score, 'mean stdev': mean_stdev, 'stdev': stdev, 'pulls': meta_policy.pulls})).sort_values(by='score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = []\n",
    "for i in range(483):\n",
    "    true_probs = scipy.stats.beta.rvs(50,50,size=num_arms)\n",
    "    policy = Policies.Thompson(10,a=50, b=50)\n",
    "    pulls = np.zeros(10)\n",
    "    rewards = np.zeros(10)\n",
    "    for i in range(10000):\n",
    "        action = policy.choice()\n",
    "        reward = np.random.binomial(1,true_probs[action])\n",
    "        pulls[action]+=1\n",
    "        rewards[action]+=reward\n",
    "        policy.getReward(action, reward)\n",
    "    regret = np.max(true_probs)*10000-pulls.dot(true_probs)\n",
    "    print(regret)\n",
    "    regrets.append(regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = np.array(regrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.std()/np.sqrt(len(regrets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_list = [Policies.ProbabilityPursuit, Policies.EmpiricalMeans, Policies.UCB, Policies.UCBmin, Policies.UCBplus, Policies.UCBVtuned, Policies.MOSS, Policies.MOSSAnytime, Policies.MOSSExperimental, Policies.klUCB, Policies.klUCBloglog, Policies.klUCBPlus, Policies.klUCBswitchAnytime, Policies.DMED, Policies.DMEDPlus, Policies.AdBandits, Policies.LM_DSEE, Policies.BESA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_names = ['ProbabilityPursuit', 'EmpiricalMeans', 'UCB', 'UCBmin', 'UCBplus', 'UCBVtuned', 'MOSS', 'MOSSAnytime', 'MOSSExperimental', 'klUCB', 'klUCBloglog', 'klUCBPlus', 'klUCBswitchAnytime', 'DMED', 'DMEDPlus', 'AdBandits', 'LM_DSEE', 'BESA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame({'Name':policy_algorithm_names, 'score': -meta_policy.rewards/(meta_policy.pulls+.0001), 'pulls': meta_policy.pulls})).sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(policy_algorithm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(policy_algorithm_list)):\n",
    "    try:\n",
    "        results.append(benchmark(policy_algorithm_list[i]))\n",
    "    except:\n",
    "        results.append(f'fail:{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Armed Bandit Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetically-Generated Tree-based Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, mean, game_length, depth, num_arms):\n",
    "        self.mean = mean\n",
    "        self.game_length = game_length\n",
    "        self.depth = depth\n",
    "        self.num_arms = num_arms\n",
    "        self.policy = klUCBPlus(num_arms)\n",
    "        self.fictitious_pulls = np.array(num_arms*[1])\n",
    "        self.fictitious_rewards = np.array(num_arms*[0])\n",
    "        self.policy.pulls = self.fictitious_pulls.copy()\n",
    "        self.policy.rewards = self.fictitious_rewards.copy()\n",
    "        if game_length == depth:\n",
    "            self.action_means = scipy.stats.bernoulli.rvs(self.mean, size=num_arms)\n",
    "        else:\n",
    "            self.action_means = scipy.stats.beta.rvs(self.mean*9+.001,(1-self.mean)*9+.001,size=num_arms)\n",
    "            # self.action_means = self.action_means*self.mean/self.action_means.mean()\n",
    "        self.next_nodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 25\n",
    "root_node = Node(mean=.5, game_length = size, depth = 1, num_arms = size)\n",
    "writer = SummaryWriter()\n",
    "reward_list = []\n",
    "big_number = 1e10\n",
    "win_condition = 100\n",
    "i = 0\n",
    "while True:\n",
    "    node=root_node\n",
    "    node_action_list = []\n",
    "    while True:\n",
    "        action = node.policy.choice()\n",
    "        node_action_list.append((node,action))\n",
    "        if node.depth == node.game_length:\n",
    "            reward = node.action_means[action]\n",
    "            writer.add_scalar('Reward', reward, str(i))\n",
    "            reward_list.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            node = node.next_nodes.setdefault(action, Node(mean=node.action_means[action], game_length=node.game_length, depth=node.depth+1, num_arms=node.num_arms))\n",
    "    node_solved = False\n",
    "    for node, action in reversed(node_action_list):\n",
    "        if node_solved !=0:\n",
    "            node.policy.pulls[action] = -node_solved * big_number\n",
    "            node.fictitious_rewards[action] \n",
    "            node.policy.pulls[action] +=1\n",
    "        else:\n",
    "            node.policy.getReward(action, (reward if node.depth%2 else 1-reward))\n",
    "        node_solved = 1 if any((node.policy.rewards==big_number)==(node.policy.pulls==big_number)) else (-1 if all(node.policy.rewards==-big_number) else 0)\n",
    "    if node_action_list[-1][0].depth % 2 == reward:\n",
    "        node_action_list[-2][0].policy.rewards[node_action_list[-2][1]] = -big_number\n",
    "    if len(reward_list)>win_condition and ((np.array(reward_list[-win_condition:])==1).all() or (np.array(reward_list[-win_condition:])==0).all()):\n",
    "        break\n",
    "    i +=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoNode():\n",
    "    def __init__(self, num_arms, legal_actions, policy_algorithm, player='black_0', state=None, fictitious_alphas = None, fictitious_betas = None):\n",
    "        self.num_arms = num_arms\n",
    "        self.legal_actions = legal_actions\n",
    "        self.player = player\n",
    "        self.state = state\n",
    "        self.policy_algorithm = policy_algorithm\n",
    "        self.policy = self.policy_algorithm(num_arms)\n",
    "        if fictitious_alphas == None and fictitious_betas == None:\n",
    "            self.fictitious_pulls = np.ones(num_arms)\n",
    "            self.fictitious_rewards = np.ones(num_arms) / 2\n",
    "        else:\n",
    "            self.fictitious_pulls = fictitious_alphas + fictitious_betas\n",
    "            self.fictitious_rewards = fictitious_alphas\n",
    "        self.policy.pulls = self.fictitious_pulls.copy()\n",
    "        self.policy.rewards = self.fictitious_rewards.copy()\n",
    "        self.policy.t = self.policy.pulls.sum()\n",
    "        self.next_nodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_algorithm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(policy_algorithm):\n",
    "    env = go_v5.env(board_size = 3, komi = 3.5)\n",
    "    env.reset(seed=42)\n",
    "    root_node = GoNode(num_arms = env.last()[0]['action_mask'].sum(), legal_actions = env.last()[0]['action_mask'].nonzero()[0], policy_algorithm=policy_algorithm)\n",
    "    winner_list = []\n",
    "    unique_nodes = 0\n",
    "    big_number = 1e10\n",
    "    win_condition = 100\n",
    "    i = 0\n",
    "    while True:\n",
    "        node = root_node\n",
    "        node_action_list = []\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        mask = observation[\"action_mask\"]\n",
    "        for agent in env.agent_iter():\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                winner = reward if agent=='black_0' else -reward\n",
    "                winner = (winner + 1)/2\n",
    "                winner_list.append(winner)\n",
    "                break\n",
    "            policy_choice = node.policy.choice()\n",
    "            action = node.legal_actions[policy_choice]\n",
    "            node_action_list.append((node,policy_choice))\n",
    "            env.step(action)\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            mask = observation[\"action_mask\"]\n",
    "            if policy_choice not in node.next_nodes:\n",
    "                unique_nodes +=1\n",
    "            node = node.next_nodes.setdefault(policy_choice, GoNode(num_arms=np.count_nonzero(observation['action_mask']), legal_actions = mask.nonzero()[0], policy_algorithm=policy_algorithm, player = 'white_0' if node.player=='black_0' else 'black_0'))\n",
    "        env.close()\n",
    "\n",
    "        if {'black_0':1,'white_0':0}[node_action_list[-1][0].player] == winner:\n",
    "            node_solved=-1\n",
    "        else:\n",
    "            node_solved=1\n",
    "        for node, action in reversed(node_action_list):\n",
    "            if node_solved !=0:\n",
    "                # node.policy.rewards[action] = -node_solved * big_number\n",
    "                # node.policy.pulls[action] +=1\n",
    "                # node.policy.t +=1\n",
    "                node.policy.getReward(action, -node_solved*big_number)\n",
    "            else:\n",
    "                node.policy.getReward(action, (winner if node.player=='black_0' else 1-winner))\n",
    "            node_solved = 1 if any(node.policy.rewards==big_number) else (-1 if all(node.policy.rewards==-big_number) else 0)\n",
    "        \n",
    "        if len(winner_list)>win_condition and ((np.array(winner_list[-win_condition:])==1).all() or (np.array(winner_list[-win_condition:])==0).all()):\n",
    "            break\n",
    "        env.reset(seed=42)\n",
    "        i+=1\n",
    "    return unique_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klUCBPlus_benchmark_results = []\n",
    "MOSSExperimental_benchmark_results = []\n",
    "while True:\n",
    "    if len(klUCBPlus_benchmark_results) <= len(MOSSExperimental_benchmark_results):\n",
    "        result = benchmark(Policies.klUCBPlus)\n",
    "        klUCBPlus_benchmark_results.append(result)\n",
    "        print(f'klUCBPlus: {result}')\n",
    "    else:\n",
    "        result = benchmark(Policies.MOSSExperimental)\n",
    "        MOSSExperimental_benchmark_results.append(result)\n",
    "        print(f'MOSSExperimental: {result}')\n",
    "    print(len(klUCBPlus_benchmark_results), len(MOSSExperimental_benchmark_results))\n",
    "    # bins = np.linspace(min(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), max(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), 20)\n",
    "    # plt.hist(klUCBPlus_benchmark_results, bins,alpha=.5)\n",
    "    # plt.hist(MOSSExperimental_benchmark_results, bins, alpha=.5)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), max(klUCBPlus_benchmark_results+MOSSExperimental_benchmark_results), 40)\n",
    "plt.hist(klUCBPlus_benchmark_results, bins,alpha=.5)\n",
    "plt.hist(MOSSExperimental_benchmark_results, bins, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([1,1,1,0,0])\n",
    "d = np.array([0,0,1,1,1])\n",
    "(c==1)==(d==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(node):\n",
    "    states = [node.state]\n",
    "    num_arms = len(root_node.state[0].flatten())+1\n",
    "    a = np.zeros(num_arms)\n",
    "    a[node.legal_actions] = node.policy.rewards\n",
    "    alphas = [a]\n",
    "    p = np.zeros(num_arms)\n",
    "    p[node.legal_actions] = node.policy.pulls\n",
    "    betas = [p-a]\n",
    "    for next_node in node.next_nodes.values():\n",
    "        next_states, next_alphas, next_betas = pull_data(next_node)\n",
    "        states += next_states\n",
    "        alphas += next_alphas\n",
    "        betas += next_betas\n",
    "    return states, alphas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "env = go_v5.env(board_size = 5, komi = 3.5)\n",
    "policy_algorithm=Policies.MOSSExperimental\n",
    "env.reset(seed=42)\n",
    "last = env.last()\n",
    "root_node = GoNode(num_arms = last[0]['action_mask'].sum(), legal_actions = last[0]['action_mask'].nonzero()[0], policy_algorithm=policy_algorithm, state = last[0]['observation'].transpose((2,1,0)))\n",
    "winner_list = []\n",
    "big_number = 1e10\n",
    "win_condition = 100\n",
    "i = 0\n",
    "using_network=False\n",
    "states_per_training_batch = 200000\n",
    "num_unique_states_visited = 1\n",
    "# data collection and training loop\n",
    "while True:\n",
    "    # data collection and tree update loop\n",
    "    while num_unique_states_visited < states_per_training_batch:\n",
    "        print(num_unique_states_visited)\n",
    "        node = root_node\n",
    "        node_action_list = []\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        state = observation['observation'].transpose((2,1,0))\n",
    "        mask = observation[\"action_mask\"]\n",
    "        # single game playout loop\n",
    "        for agent in env.agent_iter():\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "                winner = reward if agent=='black_0' else -reward\n",
    "                winner = (winner + 1)/2\n",
    "                winner_list.append(winner)\n",
    "                writer.add_scalar('Winner', winner, str(i))\n",
    "                break\n",
    "            policy_choice = node.policy.choice()\n",
    "            action = node.legal_actions[policy_choice]\n",
    "            node_action_list.append((node,policy_choice))\n",
    "            env.step(action)\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            state = observation['observation'].transpose((2,1,0))\n",
    "            mask = observation[\"action_mask\"]\n",
    "            num_arms = np.count_nonzero(mask)\n",
    "            if policy_choice in node.next_nodes:\n",
    "                node = node.next_nodes[policy_choice]\n",
    "            else:\n",
    "                fictitious_alphas, fictitious_betas = network(state) if using_network else (None, None)\n",
    "                node.next_nodes[policy_choice] = GoNode(num_arms=num_arms, \n",
    "                                                        legal_actions = mask.nonzero()[0], \n",
    "                                                        policy_algorithm=policy_algorithm if num_arms > 3 else Policies.klUCBPlus, \n",
    "                                                        player = 'white_0' if node.player=='black_0' else 'black_0', \n",
    "                                                        state=state,\n",
    "                                                        fictitious_alphas = fictitious_alphas,\n",
    "                                                        fictitious_betas = fictitious_betas)\n",
    "                num_unique_states_visited += 1\n",
    "                node = node.next_nodes[policy_choice]\n",
    "        env.close()\n",
    "\n",
    "        if {'black_0':1,'white_0':0}[node_action_list[-1][0].player] == winner:\n",
    "            node_solved=-1\n",
    "        else:\n",
    "            node_solved=1\n",
    "        # tree update loop\n",
    "        for node, action in reversed(node_action_list):\n",
    "            if node_solved !=0:\n",
    "                node.policy.rewards[action] = big_number if node_solved==-1 else .5\n",
    "                node.policy.pulls[action]=big_number\n",
    "                # node.policy.pulls[action] +=1\n",
    "            else:\n",
    "                node.policy.getReward(action, (winner if node.player=='black_0' else 1-winner))\n",
    "            node_solved = 1 if any(node.policy.rewards==big_number) else (-1 if (all(node.policy.rewards==.5) and all(node.policy.pulls==big_number)) else 0)\n",
    "        \n",
    "        # if len(winner_list)>win_condition and ((np.array(winner_list[-win_condition:])==1).all() or (np.array(winner_list[-win_condition:])==0).all()):\n",
    "        #     break\n",
    "        if any(root_node.policy.rewards==big_number):\n",
    "            break\n",
    "        env.reset(seed=42)\n",
    "\n",
    "    states, target_alphas, target_betas = pull_data(root_node)\n",
    "    if not using_network:\n",
    "        network = Net(5)\n",
    "        using_network = True\n",
    "    train(network, states, target_alphas, target_betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, target_alphas, target_betas = pull_data(root_node)\n",
    "target_alphas, target_betas = torch.FloatTensor(np.array(target_alphas)), torch.FloatTensor(np.array(target_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_alphas.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.FloatTensor(np.array(states))\n",
    "states.element_size()*states.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "            [nn.Conv2d(32, 32, 3, 1, padding='same') for i in range(2)] + \\\n",
    "            [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = F.relu(x)\n",
    "        out = torch.flatten(self.layers[-1](x), -2, -1)\n",
    "        passes = x[:,:2,:,:].mean(axis=(-1,-2))\n",
    "        out = torch.cat((out, passes.unsqueeze(-1)), axis=-1)\n",
    "        alphas = torch.exp(out[:,0,:])\n",
    "        betas=torch.exp(out[:,1,:])\n",
    "        return alphas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.next_nodes[0].policy.pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(alphas, betas, target_alphas, target_betas):\n",
    "    s=alphas+betas\n",
    "    p_hat = alphas/(alphas+betas)\n",
    "    return (target_alphas*(1-p_hat) + (target_alphas+target_betas)*p_hat**2 + alphas*betas/(s**2*(s+1))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, states, target_alphas, target_betas):\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "    batch_size = 1024\n",
    "\n",
    "    train_target_alphas, test_target_alphas = torch.split(target_alphas, [int(len(target_alphas)*0.85), len(target_alphas) - int(len(target_alphas)*0.85)])\n",
    "    train_target_betas,  test_target_betas = torch.split(target_betas, [int(len(target_betas)*0.85), len(target_betas) - int(len(target_betas)*0.85)])\n",
    "    train_states, test_states = torch.split(states, [int(len(states)*0.85), len(states) - int(len(states)*0.85)])\n",
    "    \n",
    "    train_target_alpha_batches = torch.split(train_target_alphas, batch_size)\n",
    "    train_target_beta_batches = torch.split(train_target_betas, batch_size)\n",
    "    train_batches = torch.split(train_states, batch_size)\n",
    "        \n",
    "    for epoch in range(10):\n",
    "        print(f'epoch:{epoch}')\n",
    "        for i in range(len(train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "            aug_train_inpt, aug_train_target_alpha, aug_train_target_beta = augment(train_batches[i], train_target_alpha_batches[i], train_target_beta_batches[i])\n",
    "            alphas, betas = network(aug_train_inpt)\n",
    "            \n",
    "            # train_loss = loss_mse(alphas, betas, aug_train_target_alpha, aug_train_target_beta)\n",
    "            train_loss = loss4(alphas, betas, aug_train_target_alpha, aug_train_target_beta)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 40 == 0:\n",
    "                print(train_loss.item())\n",
    "        test_alphas, test_betas = network(test_states)\n",
    "        # val_loss = loss_mse(test_alphas, test_betas, test_target_alphas, test_target_betas)\n",
    "        val_loss = loss4(test_alphas, test_betas, test_target_alphas, test_target_betas)\n",
    "        print(f'val loss:{val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(5)\n",
    "train(net, torch.FloatTensor(states), torch.FloatTensor(target_alphas), torch.FloatTensor(target_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function to choose + cross-validate loss and iterations and network architecture other inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Net, self).__init__()\n",
    "        if size == 1:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 2:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same') for _ in range(2)] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 3:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 32, 3, 1, padding='same') for _ in range(4)] + \\\n",
    "                [nn.Conv2d(32, 2, 3, 1, padding='same')])\n",
    "        elif size == 4:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 64, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(64, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 16, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(16, 2, 3, 1, padding='same')])\n",
    "        elif size == 5:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [nn.Conv2d(17, 128, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(128, 64, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(64, 32, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(32, 16, 3, 1, padding='same')] + \\\n",
    "                [nn.Conv2d(16, 2, 3, 1, padding='same')])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i](x)\n",
    "            x = F.relu(x)\n",
    "        out = torch.flatten(self.layers[-1](x), -2, -1)\n",
    "        passes = x[:,:2,:,:].mean(axis=(-1,-2))\n",
    "        out = torch.cat((out, passes.unsqueeze(-1)), axis=-1)\n",
    "        alphas = torch.exp(out[:,0,:])\n",
    "        betas=torch.exp(out[:,1,:])\n",
    "        return alphas, betas\n",
    "    \n",
    "def augment(inpt, target_alphas, target_betas):\n",
    "    rotation = random.randrange(4)\n",
    "    flip = random.randrange(2)\n",
    "    alpha_board_targets , alpha_pass_targets = torch.split(target_alphas, [9,1], dim=-1)\n",
    "    beta_board_targets , beta_pass_targets = torch.split(target_betas, [9,1], dim=-1)\n",
    "    alpha_board_targets_reshaped = alpha_board_targets.view(-1, int(np.sqrt(alpha_board_targets.shape[-1])),int(np.sqrt(alpha_board_targets.shape[-1])))\n",
    "    beta_board_targets_reshaped = beta_board_targets.view(-1, int(np.sqrt(beta_board_targets.shape[-1])),int(np.sqrt(beta_board_targets.shape[-1])))\n",
    "    inpt = torch.rot90(inpt,rotation,[-2,-1])\n",
    "    alpha_board_targets_reshaped = torch.rot90(alpha_board_targets_reshaped,rotation,[-2,-1])\n",
    "    beta_board_targets_reshaped = torch.rot90(beta_board_targets_reshaped,rotation,[-2,-1])\n",
    "    if flip:\n",
    "        inpt = torch.flip(inpt, [-1])\n",
    "        alpha_board_targets_reshaped = torch.flip(alpha_board_targets_reshaped, [-1])\n",
    "        beta_board_targets_reshaped = torch.flip(beta_board_targets_reshaped, [-1])\n",
    "    alpha_board_targets = alpha_board_targets_reshaped.reshape(-1, alpha_board_targets.shape[-1])\n",
    "    beta_board_targets = beta_board_targets_reshaped.reshape(-1, beta_board_targets.shape[-1])\n",
    "    alpha_targets = torch.cat((alpha_board_targets, alpha_pass_targets),-1).contiguous()\n",
    "    beta_targets = torch.cat((beta_board_targets, beta_pass_targets),-1).contiguous()\n",
    "    return inpt, alpha_targets, beta_targets\n",
    "\n",
    "def loss1(target_alpha_batches, p_hat, s):\n",
    "    # return (target_beta_batches*p_hat**2 + target_alpha_batches*(1-p_hat)**2 + p_hat*(1-p_hat)/(s+1)*pulls).mean()\n",
    "    return (target_alpha_batches - target_alpha_batches*p_hat + p_hat*(1-p_hat)/(s+1)).mean()\n",
    "\n",
    "def loss2(p_hat, target_probs):\n",
    "    return ((p_hat-target_probs)**2).mean()\n",
    "\n",
    "def loss3(target_beta_batches, target_alpha_batches, p_hat):\n",
    "    return (target_beta_batches*p_hat**2 + target_alpha_batches*(1-p_hat)**2).mean()\n",
    "\n",
    "def loss4(alphas, betas, target_alphas, target_betas):\n",
    "    # return (target_alpha_batches*(torch.digamma(s)-torch.digamma(alphas))).mean()\n",
    "    return ((target_alphas + target_betas)*torch.digamma(alphas+betas)-target_alphas*torch.digamma(alphas)-target_betas*torch.digamma(betas)).mean()\n",
    "\n",
    "def tuning(network, loss_func):\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-5)\n",
    "\n",
    "    batch_size = 64\n",
    "    \n",
    "    train_target_alphas, val_target_alphas, test_target_alphas = torch.split(target_alphas, [int(len(target_alphas)*0.7), int(len(target_alphas)*0.15), int(len(target_alphas)*0.15) + 1])\n",
    "    train_target_betas, val_target_betas, test_target_betas = torch.split(target_betas, [int(len(target_betas)*0.7), int(len(target_betas)*0.15), int(len(target_betas)*0.15) + 1])\n",
    "    train_states, val_states, test_states = torch.split(states, [int(len(states)*0.7), int(len(states)*0.15), int(len(states)*0.15) + 1])\n",
    "    \n",
    "    train_target_alpha_batches = torch.split(train_target_alphas, batch_size)\n",
    "    train_target_beta_batches = torch.split(train_target_betas, batch_size)\n",
    "    train_batches = torch.split(train_states, batch_size)\n",
    "    ma = 0\n",
    "        \n",
    "    for epoch in range(10):\n",
    "        print(f'epoch:{epoch}')\n",
    "        for i in range(len(train_batches)):\n",
    "            optimizer.zero_grad()\n",
    "            aug_train_inpt, aug_train_target_alpha, aug_train_target_beta = augment(train_batches[i], train_target_alpha_batches[i], train_target_beta_batches[i])\n",
    "            alphas, betas = network(aug_train_inpt)\n",
    "\n",
    "            s = alphas+betas\n",
    "            p_hat = alphas/s\n",
    "            \n",
    "            pulls = aug_train_target_alpha + aug_train_target_beta\n",
    "            target_probs = aug_train_target_alpha/pulls\n",
    "            target_probs[target_probs != target_probs] = 0\n",
    "\n",
    "            if loss_func == loss1:\n",
    "                train_loss = loss1(aug_train_target_alpha, p_hat, s)\n",
    "            elif loss_func == loss2:\n",
    "                train_loss = loss2(p_hat, target_probs)\n",
    "            elif loss_func == loss3:\n",
    "                train_loss = loss3(aug_train_target_beta, aug_train_target_alpha, p_hat)\n",
    "            elif loss_func == loss4:\n",
    "                train_loss = loss4(aug_train_target_alpha, s, alphas)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 500 == 0:\n",
    "                print(train_loss.item())\n",
    "    \n",
    "    val_target_alpha_batches, val_target_beta_batches = torch.split(val_target_alphas, batch_size), torch.split(val_target_betas, batch_size)\n",
    "    val_batches = torch.split(val_states, batch_size)\n",
    "    val_batch_loss = []\n",
    "    for i in range(len(val_batches)):\n",
    "        optimizer.zero_grad()\n",
    "        alphas, betas = network(val_batches[i])\n",
    "\n",
    "        s = alphas+betas\n",
    "        p_hat = alphas/s\n",
    "        \n",
    "        pulls = val_target_alpha_batches[i] + val_target_beta_batches[i]\n",
    "        target_probs = val_target_alpha_batches[i]/pulls\n",
    "        target_probs[target_probs != target_probs] = 0\n",
    "\n",
    "        val_batch_loss += [loss1(val_target_alpha_batches[i], p_hat, s).item()]\n",
    "\n",
    "    return val_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "for architecture, loss_func in itertools.product(*[[Net(4), Net(5)],\n",
    "                                     [loss1, loss2, loss3, loss4]]):\n",
    "    val_losses.append(tuning(architecture, loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in val_losses:\n",
    "    print(sum(x)/352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i = 0\n",
    "best_error = np.Inf\n",
    "for i in range(len(val_losses)):\n",
    "    if np.mean(val_losses[i]) < best_error:\n",
    "        best_i = i\n",
    "        best_error = np.mean(val_losses[i])\n",
    "print(i)\n",
    "print(best_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_target = torch.randn(64,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_targets , pass_targets = torch.split(fake_target, [9,1], dim=-1)\n",
    "board_targets_reshaped = board_targets.view(-1, int(np.sqrt(board_targets.shape[-1])),int(np.sqrt(board_targets.shape[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.sqrt(board_targets.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = torch.zeros(64,17,3,3)\n",
    "inpt[0,0,0,0]=1\n",
    "targets = torch.zeros(64,10)\n",
    "targets[0,0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment(inpt, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.split(a,64)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network(torch.FloatTensor(root_node.next_nodes[4].state).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.next_nodes[4].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
